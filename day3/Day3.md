#  Day 3 Completed â€“ Databricks 14 Days AI Challenge  
**Sponsored by Databricks**

Successfully completed **Day 3** of the **Databricks AI Challenge**, where I explored **advanced PySpark concepts** and learned how Spark handles large-scale data transformations efficiently 

---

##  What I Learned Today

- **PySpark vs Pandas**
  - Pandas works in-memory on a single machine
  - PySpark works in a distributed manner across clusters
- **Joins in Spark**
  - Inner Join
  - Left Join
  - Right Join
  - Outer Join
- **Window Functions**
  - Running totals
  - Ranking and ordering data
- **User-Defined Functions (UDFs)**
  - Creating custom logic in Spark
  - Applying Python functions to DataFrames

---

##  Tasks I Completed

- Loaded the **full e-commerce dataset**
- Performed **complex joins** between multiple DataFrames
- Calculated **running totals** using Spark window functions
- Created **derived features** for further analysis

---

##  Key Takeaways

- PySpark is suitable for **big data processing**, unlike Pandas
- Joins allow combining datasets efficiently at scale
- Window functions help analyze trends without collapsing rows
- UDFs enable custom business logic when built-in functions are insufficient

---

##  Hashtags

`#DatabricksWithIDC`  
`#14DaysAIChallenge`  
`#PySpark`  
`#BigData`  
`#LearningInPublic`

---
## ðŸ“¸ Screenshot / Proof

![Day 2  Progress](../images/day3.png)
