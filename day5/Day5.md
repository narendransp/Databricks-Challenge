## **Day 5 Completed â€“ Databricks 14 Days AI Challenge**  
**Sponsored by Databricks**

Successfully completed **Day 5**, where I explored **advanced Delta Lake concepts** and learned how modern data lakes support reliable, scalable, and high-performance data engineering workflows.

---

### **What I Learned Today**

**Delta Lake Time Travel**
- Accessed historical versions of data  
- Used versioning for debugging and auditing changes  

**MERGE Operations (Upserts)**
- Performed insert, update, and delete operations in a single statement  
- Built efficient incremental data pipelines  

**OPTIMIZE & ZORDER**
- Reduced small file issues  
- Improved query performance for selective queries  

**VACUUM**
- Removed obsolete files  
- Managed storage while maintaining required data history  

---

### **Tasks I Completed**

- Implemented **incremental MERGE** logic for Delta tables  
- Queried **historical data versions** using time travel  
- Applied **OPTIMIZE and ZORDER** for performance tuning  
- Executed **VACUUM** to clean up unused files  

---

### **Key Takeaways**

- Delta Lake brings **ACID transactions** to data lakes  
- MERGE simplifies complex ETL and CDC pipelines  
- Time travel improves reliability and traceability  
- Optimization is critical for production-scale data systems  

---

### **Hashtags**

#DatabricksWithIDC  
#14DaysAIChallenge  
#DeltaLake  
#DataEngineering  
#BigData  
#LearningInPublic  

## ðŸ“¸ Screenshot / Proof

![Day 1 Progress](../images/day5.png)
